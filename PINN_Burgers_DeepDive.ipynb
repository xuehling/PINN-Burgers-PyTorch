{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks (PINN) 求解 Burgers 方程\n",
    "\n",
    "## 1. 物理背景\n",
    "我们要求解的是一维 Burgers 方程，它是流体力学中对流扩散方程的简化形式，常用于模拟激波（Shock Waves）的形成。\n",
    "\n",
    "**方程形式：**\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "* $u(x, t)$：流体速度\n",
    "* $\\nu$：运动粘度系数（本例取 $\\nu = 0.01/\\pi$）\n",
    "* 域：$x \\in [-1, 1], \\quad t \\in [0, 1]$\n",
    "\n",
    "**初始与边界条件：**\n",
    "* **IC ($t=0$):** $u(x, 0) = -\\sin(\\pi x)$\n",
    "* **BC ($x=\\pm 1$):** $u(-1, t) = u(1, t) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 核心数学推导：自动微分与张量缩并\n",
    "\n",
    "在 PINN 的代码实现中，最核心的一行代码是：\n",
    "```python\n",
    "grads = torch.autograd.grad(u, inputs, torch.ones_like(u), create_graph=True)[0]\n",
    "```\n",
    "\n",
    "为了理解这一步发生了什么，我们需要从张量分析的角度来拆解 **Vector-Jacobian Product (VJP)** 的过程。\n",
    "\n",
    "### 2.1 定义变量与形状\n",
    "假设我们有 $N$ 个采样点（例如 $N=2000$）：\n",
    "* **输入 (Input)** $\\mathbf{X}$：形状为 $[N, 2]$。第 $q$ 行第 $m$ 列元素 $X_{qm}$ 代表第 $q$ 个样本的第 $m$ 个特征（$m=1$为$x$, $m=2$为$t$）。\n",
    "* **输出 (Output)** $\\mathbf{u}$：形状为 $[N, 1]$。第 $p$ 行元素 $u_p$ 代表第 $p$ 个样本的预测速度。\n",
    "\n",
    "### 2.2 隐形的三阶雅可比张量\n",
    "理论上，输出 $\\mathbf{u}$ 对输入 $\\mathbf{X}$ 的导数是一个三阶张量 $\\mathcal{J}$，形状为 $[N, N, 2]$：\n",
    "$$\n",
    "\\mathcal{J}_{pqm} = \\frac{\\partial u_p}{\\partial X_{qm}}\n",
    "$$\n",
    "\n",
    "利用神经网络的 **样本独立性 (Batch Independence)**：第 $p$ 个样本的输出只取决于第 $p$ 个样本的输入。当 $p \\neq q$ 时，导数为 0。我们可以引入克罗内克符号 $\\delta_{pq}$：\n",
    "\n",
    "$$\n",
    "\\mathcal{J}_{pqm} = \\frac{\\partial u_p}{\\partial X_{pm}} \\cdot \\delta_{pq}\n",
    "$$\n",
    "\n",
    "这是一个**稀疏张量**，只有对角线平面（$p=q$）上有值。\n",
    "\n",
    "### 2.3 `ones_like(u)` 与张量“拍扁” (Flattening)\n",
    "我们在 `grad` 函数中传入的 `grad_outputs` 是 $\\mathbf{v} = \\text{ones\\_like}(u)$，这是一个形状为 $[N, 1]$ 的全 1 向量。\n",
    "PyTorch 执行的是 **张量缩并 (Tensor Contraction)** 操作：\n",
    "\n",
    "$$\n",
    "R_{qm} = \\sum_{p=1}^{N} v_p \\cdot \\mathcal{J}_{pqm}\n",
    "$$\n",
    "\n",
    "由于 $v_p = 1$，且 $\\delta_{pq}$ 只有在 $p=q$ 时为 1，求和号 $\\sum_p$ 会把所有 $p \\neq q$ 的项（都是0）过滤掉，只保留 $p=q$ 的项。\n",
    "\n",
    "**几何直观：**\n",
    "这相当于用手掌（向量 $\\mathbf{v}$）沿着输出样本维度（$p$轴）垂直向下压，把三阶张量 $\\mathcal{J}$ **“拍扁”** 成了二维矩阵。\n",
    "\n",
    "### 2.4 最终结果\n",
    "$$\n",
    "R_{qm} = \\frac{\\partial u_q}{\\partial X_{qm}}\n",
    "$$\n",
    "结果 $\\mathbf{R}$ 的形状是 $[N, 2]$，恰好对应了我们需要计算的 $u_x$ 和 $u_t$。这也是为什么我们需要后续进行切片操作 `[:, 0:1]` 和 `[:, 1:2]` 的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- 配置 ---\n",
    "# 使用 GPU 如果可用，否则使用 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "NU = 0.01 / torch.pi  # 粘度系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        # 输入 2 维 (x, t)，输出 1 维 (u)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 初始化模型与优化器\n",
    "model = PINN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pde_data(n=2000):\n",
    "    \"\"\"生成物理方程所需的内部采样点 (Collocation Points)\"\"\"\n",
    "    # 1. torch.rand 生成 [0, 1)\n",
    "    # 2. * 2 - 1 将范围映射到 [-1, 1)\n",
    "    x = (torch.rand(n, 1) * 2 - 1).to(device)\n",
    "    t = (torch.rand(n, 1) * 1).to(device)\n",
    "    \n",
    "    # 【关键】拼接成 [N, 2]，并开启梯度追踪\n",
    "    inputs = torch.cat([x, t], dim=1).requires_grad_(True)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"开始训练 Burgers' Equation...\")\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for step in range(5001):\n",
    "    \n",
    "    # --- Part A: PDE Loss (物理方程残差) ---\n",
    "    inputs = get_pde_data()\n",
    "    u = model(inputs)\n",
    "    \n",
    "    # 第一次求导：得到 [u_x, u_t]\n",
    "    # grad_outputs=torch.ones_like(u) 即上文提到的权重向量 v\n",
    "    grads = torch.autograd.grad(u, inputs, torch.ones_like(u), create_graph=True)[0]\n",
    "    \n",
    "    # 切片分离导数\n",
    "    u_x = grads[:, 0:1]\n",
    "    u_t = grads[:, 1:2]\n",
    "    \n",
    "    # 第二次求导：得到 u_xx\n",
    "    grads_2 = torch.autograd.grad(u_x, inputs, torch.ones_like(u_x), create_graph=True)[0]\n",
    "    u_xx = grads_2[:, 0:1]\n",
    "    \n",
    "    # Burgers 方程残差: f = u_t + u*u_x - nu*u_xx\n",
    "    f = u_t + u * u_x - NU * u_xx\n",
    "    loss_pde = torch.mean(f**2)\n",
    "    \n",
    "    # --- Part B: Initial Condition Loss (IC) ---\n",
    "    # t = 0, x ∈ [-1, 1], u = -sin(πx)\n",
    "    x_ic = (torch.rand(500, 1) * 2 - 1).to(device)\n",
    "    t_ic = torch.zeros(500, 1).to(device)\n",
    "    inputs_ic = torch.cat([x_ic, t_ic], dim=1)\n",
    "    \n",
    "    u_ic_pred = model(inputs_ic)\n",
    "    u_ic_true = -torch.sin(torch.pi * x_ic)\n",
    "    loss_ic = torch.mean((u_ic_pred - u_ic_true)**2)\n",
    "    \n",
    "    # --- Part C: Boundary Condition Loss (BC) ---\n",
    "    # x = ±1, t ∈ [0, 1], u = 0\n",
    "    t_bc = torch.rand(500, 1).to(device)\n",
    "    x_bc_left = -1 * torch.ones(500, 1).to(device)\n",
    "    x_bc_right = 1 * torch.ones(500, 1).to(device)\n",
    "    \n",
    "    inputs_bc = torch.cat([\n",
    "        torch.cat([x_bc_left, t_bc], dim=1),\n",
    "        torch.cat([x_bc_right, t_bc], dim=1)\n",
    "    ], dim=0)\n",
    "    \n",
    "    u_bc_pred = model(inputs_bc)\n",
    "    loss_bc = torch.mean(u_bc_pred**2) # u should be 0\n",
    "    \n",
    "    # --- Part D: Backpropagation ---\n",
    "    loss = loss_pde + loss_ic + loss_bc\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step}: Total Loss={loss.item():.6f} \"\n",
    "              f\"(PDE={loss_pde.item():.5f}, IC={loss_ic.item():.5f}, BC={loss_bc.item():.5f})\")\n",
    "\n",
    "print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 绘图准备 ---\n",
    "x_np = np.linspace(-1, 1, 100)\n",
    "t_np = np.linspace(0, 1, 100)\n",
    "X, T = np.meshgrid(x_np, t_np) # 生成网格用于画图\n",
    "\n",
    "# 展平并拼接，转为 Tensor\n",
    "x_flat = X.flatten()[:, None]\n",
    "t_flat = T.flatten()[:, None]\n",
    "inputs_test = torch.from_numpy(np.hstack((x_flat, t_flat))).float().to(device)\n",
    "\n",
    "# 预测\n",
    "u_pred = model(inputs_test).cpu().detach().numpy()\n",
    "U = u_pred.reshape(100, 100)\n",
    "\n",
    "# --- 绘制热力图 ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pcolormesh(T, X, U, cmap='jet', shading='auto')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('u(x, t)')\n",
    "plt.xlabel('t (Time)')\n",
    "plt.ylabel('x (Space)')\n",
    "plt.title(\"Burgers' Equation Solution via PINN\")\n",
    "\n",
    "# 验证激波位置\n",
    "plt.text(0.5, 0.5, 'Shock Formation', color='white', ha='center', fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# --- 绘制 Loss 曲线 ---\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(loss_history)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Training Loss (Log Scale)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}